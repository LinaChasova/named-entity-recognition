{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve all file names\n",
    "\n",
    "Firstly, I need to get all corpus in one place. As it all inside different folders, we go through directories to retrieve all documents' names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:25:17.014889Z",
     "start_time": "2020-02-28T09:25:17.004168Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_files_path(path:str, extension: str):\n",
    "    files_path = []\n",
    "\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if extension in file:\n",
    "                files_path.append(os.path.join(r, file))\n",
    "                \n",
    "    return files_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:25:17.898570Z",
     "start_time": "2020-02-28T09:25:17.873674Z"
    }
   },
   "outputs": [],
   "source": [
    "files_path = read_files_path(\"entval-events/\", '.txt')\n",
    "files_path_ann = read_files_path(\"entval-events/\", '.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:25:18.728749Z",
     "start_time": "2020-02-28T09:25:18.718968Z"
    }
   },
   "outputs": [],
   "source": [
    "files_path = sorted(files_path)\n",
    "files_path_ann = sorted(files_path_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of document corpus\n",
    "\n",
    "Secondly, I need to detect sentences in the document.\n",
    "\n",
    "Pipeline:\n",
    "- get all documents and its id in dictionary\n",
    "- detect sentences in all documents\n",
    "- tokenize sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all documents and its id in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:25:27.824163Z",
     "start_time": "2020-02-28T09:25:27.815433Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    documents - dictionary {doc_id: text}\n",
    "    map_ids   - dictionary {doc_id: path}\n",
    "'''\n",
    "\n",
    "def read_documents(files_path):\n",
    "\n",
    "    documents = {}\n",
    "    map_ids = {}\n",
    "\n",
    "    doc_id = 0\n",
    "    for file_path in files_path:\n",
    "        with open(file_path, 'r') as fh:\n",
    "            map_ids[doc_id] = file_path\n",
    "            documents[doc_id] = fh.read()\n",
    "        doc_id += 1\n",
    "        \n",
    "    return documents, map_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:25:29.622789Z",
     "start_time": "2020-02-28T09:25:28.422126Z"
    }
   },
   "outputs": [],
   "source": [
    "documents, map_ids = read_documents(files_path)\n",
    "documents_ann, map_ids_ann = read_documents(files_path_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Detect sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:25:44.161080Z",
     "start_time": "2020-02-28T09:25:41.284063Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    sentences_index - dictionary {doc_id: [sentence_1, sentence_2, ...]}\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences_index = {}\n",
    "for doc in documents:\n",
    "\n",
    "    new_doc = re.sub(r\"(?<=\\w)(\\n[А-Я])\", r\".\\1\", documents[doc])\n",
    "\n",
    "    sentences = sent_tokenize(new_doc)\n",
    "    sentences = [sent.replace(u'\\xa0', u' ') for sent in sentences]\n",
    "    sentences = [sent.replace('\\n', ' ') for sent in sentences]\n",
    "    sentences_index[doc] = sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:25:48.435000Z",
     "start_time": "2020-02-28T09:25:48.047602Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    index_word - dictionary {doc_id_1: [sentence_1 - [word_1, word_2, ...], sentence_2, ...]}\n",
    "'''\n",
    "\n",
    "documents_output = {}\n",
    "index = {}\n",
    "for doc in sentences_index:\n",
    "    documents_output[doc] = ''\n",
    "    \n",
    "    for sentence in sentences_index[doc]:\n",
    "\n",
    "        sentence = re.sub(r'[ёЁ]', 'е', sentence)[:-1]\n",
    "        sentence = re.sub(r'[@|©] ', '', sentence)\n",
    "        sentence = re.sub(r'[,\\/:;] ', ' ', sentence)\n",
    "        \n",
    "        sentence = re.sub(r'[»«]', '\\\"', sentence)\n",
    "        sentence = re.sub(r' [\\\"\\/] ', ' ', sentence)\n",
    "        sentence = re.sub(r' — ', ' ', sentence)\n",
    "\n",
    "        words = sentence.split(' ')\n",
    "        words = list(filter(lambda a: a != '', words))\n",
    "        \n",
    "        documents_output[doc] += ' '.join(words)\n",
    "\n",
    "        if doc not in index:\n",
    "            index[doc] = []\n",
    "\n",
    "        index[doc].append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output format\n",
    "\n",
    "- T - (entity)\n",
    "- E - (event)\n",
    "- type - type of entity/event.\n",
    "- O - nothing\n",
    "- B-T/E-type - beginning of found token\n",
    "- I-T/E-type - next part of found token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T10:24:38.968191Z",
     "start_time": "2020-02-28T10:24:38.195009Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "new_docs = {}\n",
    "for doc in documents:\n",
    "\n",
    "#    new_doc = re.sub(r\"(?<=\\w)(\\n[А-Я])\", r\".\\1\", documents[doc])\n",
    "    new_doc = documents[doc]\n",
    "#     new_doc = re.sub(r'\\xa0', ' ', new_doc)\n",
    "#     new_doc = re.sub(r'\\n', ' ', new_doc)\n",
    "#     new_doc = re.sub(r'[ёЁ]', 'е', new_doc)\n",
    "#     new_doc = re.sub(r'[@|©] ', '  ', new_doc)\n",
    "#     new_doc = re.sub(r'[,\\/:;] ', '  ', new_doc)\n",
    "\n",
    "#     new_doc = re.sub(r'[»«]', '\\\"', new_doc)\n",
    "#     new_doc = re.sub(r' [\\\"\\/] ', '   ', new_doc)\n",
    "#     new_doc = re.sub(r' — ', '   ', new_doc)\n",
    "    \n",
    "    sentences = sent_tokenize(new_doc)\n",
    "    sentences = [sent + ' ' for sent in sentences]\n",
    "    \n",
    "    new_docs[doc] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T10:25:07.784502Z",
     "start_time": "2020-02-28T10:25:07.770466Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_lengths = {}\n",
    "for doc, sentences in new_docs.items():\n",
    "    sum_ = 0\n",
    "    sent_lengths[doc] = []\n",
    "    for sentence in sentences:\n",
    "        sum_ += len(sentence)\n",
    "        sent_lengths[doc].append(sum_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T10:25:09.199410Z",
     "start_time": "2020-02-28T10:25:09.193811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74,\n",
       " 400,\n",
       " 690,\n",
       " 737,\n",
       " 861,\n",
       " 1008,\n",
       " 1197,\n",
       " 1357,\n",
       " 1429,\n",
       " 1469,\n",
       " 1562,\n",
       " 1623,\n",
       " 1689,\n",
       " 1812,\n",
       " 1995,\n",
       " 2229]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:26:02.365927Z",
     "start_time": "2020-02-28T09:26:02.193069Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    ann_index - DataFrame; [doc_id, start_id, end_id, words, type]\n",
    "'''\n",
    "\n",
    "doc_ids = []\n",
    "start_ids = []\n",
    "end_ids = []\n",
    "words = []\n",
    "types = []\n",
    "\n",
    "for doc in documents_ann:\n",
    "    entities = documents_ann[doc].split('\\n')\n",
    "    for entity in entities:\n",
    "        ent = entity.split('\\t')\n",
    "\n",
    "        if ent == [''] or 'A' in ent[0] or '#' in ent[0]:\n",
    "            continue\n",
    "\n",
    "        # change entity token to an event token\n",
    "        if 'E' in ent[0]:\n",
    "            types[-1] = 'E' + types[-1][1:]\n",
    "            continue\n",
    "\n",
    "        info = ent[1].split(' ')\n",
    "        # parse type\n",
    "        types.append('T-' + info[0])\n",
    "        # add doc id\n",
    "        doc_ids.append(doc)      \n",
    "        # add words\n",
    "        words.append(ent[2])\n",
    "\n",
    "        # parse start end indices\n",
    "        start_ids.append(int(info[1]))\n",
    "        try:\n",
    "            end_ids.append(int(info[2]))\n",
    "        except:\n",
    "            end_ids.append(int(info[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:26:09.359410Z",
     "start_time": "2020-02-28T09:26:04.687597Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ann_index = pd.DataFrame([doc_ids, start_ids, end_ids, words, types],\n",
    "                         index=['doc', 'start', 'end', 'word', 'type'])\n",
    "ann_index = ann_index.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T09:26:16.813975Z",
     "start_time": "2020-02-28T09:26:16.772343Z"
    }
   },
   "outputs": [],
   "source": [
    "# sort by start id\n",
    "ann_index = ann_index.sort_values(['doc', 'start'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T15:52:27.632565Z",
     "start_time": "2020-02-17T15:52:27.446688Z"
    }
   },
   "outputs": [],
   "source": [
    "ann_index.to_csv('check.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T12:01:50.905157Z",
     "start_time": "2020-02-28T12:01:19.040440Z"
    },
    "code_folding": [
     40
    ]
   },
   "outputs": [],
   "source": [
    "def add_words(u_w, di, si, tag='O'):\n",
    "    # split string on words by space\n",
    "    u_w = u_w.split(' ')\n",
    "    u_w = list(filter(lambda a: a != '' and a != '\\xa0' and a != '\\n', u_w))\n",
    "\n",
    "    upd_u_w = []\n",
    "    for word in u_w:\n",
    "        upd_word = list(\n",
    "            filter(None, re.split(r\"([.\\/@|©,\\/:;»«\\\"\\/—()\\n\\xa0])\", word)))\n",
    "        upd_word = list(\n",
    "            filter(lambda a: a != '' and a != '\\xa0' and a != '\\n', upd_word))\n",
    "\n",
    "        for upd_w in upd_word:\n",
    "            doc_ids.append(di)\n",
    "            sentence_ids.append(si)\n",
    "\n",
    "            words.append(upd_w)\n",
    "\n",
    "            if tag != 'O':\n",
    "                tagge = 'B-' + tag if i == 0 else 'I-' + tag\n",
    "            else:\n",
    "                tagge = tag\n",
    "\n",
    "            types.append(tagge)\n",
    "\n",
    "\n",
    "doc_ids = []\n",
    "sentence_ids = []\n",
    "words = []\n",
    "types = []\n",
    "\n",
    "last_doc_id = -1\n",
    "last_end = -1\n",
    "last_sent_id = -1\n",
    "for index, row in ann_index.iterrows():\n",
    "    # doc_ids.append(cur_doc_id)\n",
    "\n",
    "    # get relative index of tagged words in sentence\n",
    "    for i, length in enumerate(sent_lengths[row.doc]):\n",
    "        if row.start < length:\n",
    "            sent_id = i\n",
    "            relative_start = row.start - sent_lengths[row.doc][\n",
    "                i - 1] if i != 0 else row.start\n",
    "            relative_end = row.end - sent_lengths[row.doc][\n",
    "                i - 1] if i != 0 else row.end\n",
    "            break\n",
    "\n",
    "    if last_doc_id == row.doc and last_sent_id == sent_id and relative_start < last_end:\n",
    "        continue\n",
    "\n",
    "#     print('last doc:', last_doc_id, 'last_end:', last_end, 'last_sent_id:',\n",
    "#           last_sent_id, 'sent_id:', sent_id, 'start:', row.start,\n",
    "#           relative_start, 'length:', sent_lengths[row.doc][sent_id], 'word:',\n",
    "#           row.word)\n",
    "\n",
    "#     if row.doc == 1:\n",
    "#         break\n",
    "\n",
    "# get all untagged words\n",
    "# if in the same doc\n",
    "    if last_doc_id == row.doc:\n",
    "        # if the same sentence untagged words start from ending\n",
    "        # of last tagged word till start of current one\n",
    "        if last_sent_id == sent_id:\n",
    "            untagged_words = new_docs[\n",
    "                row.doc][sent_id][last_end:relative_start]\n",
    "            add_words(untagged_words, row.doc, sent_id)\n",
    "        # if different sentence untagged words are from the ending\n",
    "        # of last tagged word till the end of last sentence\n",
    "        # and from begginning of current sentence till the start\n",
    "        # of current tagged words\n",
    "        else:\n",
    "            untagged_words = new_docs[row.doc][last_sent_id][last_end:]\n",
    "            add_words(untagged_words, row.doc, last_sent_id)\n",
    "\n",
    "            s_i = last_sent_id + 1\n",
    "            while s_i < sent_id:\n",
    "                untagged_words = new_docs[row.doc][s_i]\n",
    "                add_words(untagged_words, row.doc, s_i)\n",
    "                s_i += 1\n",
    "\n",
    "            untagged_words = new_docs[row.doc][sent_id][:relative_start]\n",
    "            add_words(untagged_words, row.doc, sent_id)\n",
    "    # if not the same doc, untagged words start from the ending of last tagged\n",
    "    # word in the last sentence till the end of doc\n",
    "    # and from the beginning of sentence till the start of tagged words\n",
    "    else:\n",
    "        # if last end is equal to -1, it is the first doc\n",
    "        if last_end != -1:\n",
    "            untagged_words = new_docs[last_doc_id][last_sent_id][last_end:]\n",
    "            add_words(untagged_words, last_doc_id, last_sent_id)\n",
    "\n",
    "            l_s_i = len(new_docs[last_doc_id])\n",
    "            s_i = last_sent_id + 1\n",
    "            while s_i < l_s_i:\n",
    "                untagged_words = new_docs[last_doc_id][s_i]\n",
    "                add_words(untagged_words, last_doc_id, s_i)\n",
    "                s_i += 1\n",
    "\n",
    "            d_i = last_doc_id + 1\n",
    "            while d_i < row.doc:\n",
    "                s_i = 0\n",
    "                l_s_i = len(new_docs[last_doc_id])\n",
    "                while s_i < l_s_i:\n",
    "                    untagged_words = new_docs[d_i][s_i]\n",
    "                    add_words(untagged_words, d_i, s_i)\n",
    "                    s_i += 1\n",
    "                d_i += 1\n",
    "\n",
    "        s_i = 0\n",
    "        while s_i != sent_id:\n",
    "            untagged_words = new_docs[row.doc][s_i]\n",
    "            add_words(untagged_words, row.doc, s_i)\n",
    "            s_i += 1\n",
    "        untagged_words = new_docs[row.doc][sent_id][:relative_start]\n",
    "        add_words(untagged_words, row.doc, sent_id)\n",
    "\n",
    "    tagged_words = new_docs[row.doc][sent_id][relative_start:relative_end]\n",
    "    add_words(tagged_words, row.doc, sent_id, tag=row.type)\n",
    "\n",
    "    # save sentence id of the last occurence of tagged words\n",
    "    last_sent_id = sent_id\n",
    "    last_doc_id = row.doc\n",
    "    last_end = relative_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T12:02:31.903706Z",
     "start_time": "2020-02-28T12:02:02.078319Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([doc_ids, sentence_ids, words, types],\n",
    "                  index=['doc', 'sentence', 'word', 'type'])\n",
    "df = df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T12:02:45.407013Z",
     "start_time": "2020-02-28T12:02:44.644471Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-28T10:14:01.232213Z",
     "start_time": "2020-02-28T10:14:01.225811Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc         1\n",
       "sentence    6\n",
       "word        а\n",
       "type        O\n",
       "Name: 441, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[441]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
