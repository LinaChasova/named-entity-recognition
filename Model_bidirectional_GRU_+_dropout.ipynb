{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Model bidirectional GRU + dropout.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7p8bvYcnXeGz"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAj9c0APbziB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d=[]\n",
        "while(1):\n",
        "  d.append('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gveZkRlUR9rg",
        "colab_type": "code",
        "outputId": "14472beb-9d4a-44a1-d2cf-0ac0f8191e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJxvjkqoSiyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path to the embeddings file\n",
        "embed_path = '/content/drive/My Drive/cc.ru.300.bin'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sqVzESG68Cu",
        "colab_type": "text"
      },
      "source": [
        "# Word embeddings\n",
        "\n",
        "## Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGS9JBht68Cy",
        "colab_type": "code",
        "outputId": "ddf1b8ff-8f15-46c8-c9e0-ec1b11ca45d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!pip install fasttext\n",
        "\n",
        "import fasttext.util\n",
        "import fasttext\n",
        "\n",
        "ft = fasttext.load_model(embed_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2385798 sha256=74b7f23f8884a8eb14adcc0717ffb429ccbb7934a70502c728d0386f94903ee8\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iigHQlBRUv0R",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9wyutRqUyx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "from datetime import datetime\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnNtU6WzRg_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    torch.device('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rykt3cg-duH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# constants\n",
        "EMBED_DIM = 300\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 50\n",
        "PAD_WORD = '`'\n",
        "PAD_TAG = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs8M_ehwlCfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(state, is_best, checkpoint_dir, best_model_dir):\n",
        "    f_path = 'models/' + checkpoint_dir + '_model.pt'\n",
        "    if not os.path.exists('models'):\n",
        "        os.makedirs('models')\n",
        "\n",
        "    torch.save(state, f_path)\n",
        "    if is_best:\n",
        "        best_fpath = best_model_dir + '/best_model.pt'\n",
        "        if not os.path.exists(best_model_dir):\n",
        "            os.makedirs(best_model_dir)\n",
        "\n",
        "        shutil.copyfile(f_path, best_fpath)\n",
        "\n",
        "\n",
        "def load_model(checkpoint_fpath, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, optimizer, checkpoint['epoch']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKIaXQOZjza1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Corpus(Dataset):\n",
        "    def __init__(self, word_to_ix, tag_to_ix, data):\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence, tags = self.data[idx]\n",
        "        return ' '.join(sentence), ' '.join(tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XcBL_XzkMzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Event_tagger(nn.Module):\n",
        "    def __init__(self,\n",
        "                 word_to_ix,\n",
        "                 tag_to_ix,\n",
        "                 ix_to_tag,\n",
        "                 weights_matrix,\n",
        "                 batch_size,\n",
        "                 word_embed_dim=300,\n",
        "                 bidirectional=True,\n",
        "                 gru_num_layers=2,\n",
        "                 dropout=.5):\n",
        "        '''\n",
        "            initialize models\n",
        "            batch_size      - size of batches for traininig\n",
        "            word_embed_dim  - dimension of word embeddings\n",
        "            gru_num_layers - number of gru layers\n",
        "            dropout         - rate for dropout layer\n",
        "        '''\n",
        "        super(Event_tagger, self).__init__()\n",
        "\n",
        "        # dictionaries\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.ix_to_tag = ix_to_tag\n",
        "\n",
        "        # parameters\n",
        "        self.batch_size = batch_size\n",
        "        self.word_embed_dim = word_embed_dim\n",
        "        self.gru_hidden_dim = self.word_embed_dim\n",
        "        self.gru_num_layers = gru_num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.word_embeds, num_embeds = create_embed_layer(weights_matrix, True)\n",
        "\n",
        "        self.gru = nn.GRU(self.word_embed_dim,\n",
        "                            self.gru_hidden_dim,\n",
        "                            dropout=self.dropout,\n",
        "                            bidirectional=bidirectional,\n",
        "                            num_layers=self.gru_num_layers,\n",
        "                            batch_first=True)\n",
        "        self.dense = nn.Linear(self.gru_hidden_dim * self.num_directions,\n",
        "                               len(self.tag_to_ix))\n",
        "        \n",
        "    def forward(self, words_batch):\n",
        "        '''\n",
        "            words_batch - contains indices of words in current batch with shape\n",
        "                        (batch_size, num_words_in_sentence)\n",
        "\n",
        "            creates word level word embeddings from words_batch\n",
        "\n",
        "            runs Bi-directional gru over input word representation to get\n",
        "            final word representation which is fed to linear layer and softmax\n",
        "            activation function to generate probability distribution for event\n",
        "            tag set\n",
        "        '''\n",
        "        # create word-level word embeddings\n",
        "        word_embed_word_level = words_batch.view(-1)\n",
        "        word_embed_word_level = self.word_embeds(word_embed_word_level)\n",
        "        batch_sent_embed = word_embed_word_level.view(words_batch.shape[0], -1,\n",
        "                                                 word_embed_word_level.shape[-1])\n",
        "\n",
        "        # create final word representation from gru\n",
        "        gru_out, _ = self.gru(batch_sent_embed)\n",
        "\n",
        "        # get probabilities for event tag\n",
        "        tag_space = self.dense(gru_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j9gMl5bXpcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_vocabulary(df):\n",
        "    '''\n",
        "        creates vocabulary from dataset\n",
        "    '''\n",
        "    all_words = list(set(list(df.word)))\n",
        "    word_to_ix = {all_words[ix]:ix for ix in range(len(all_words))}\n",
        "\n",
        "    all_tags = list(set(list(df.type)))\n",
        "    tag_to_ix = {all_tags[ix]:ix for ix in range(len(all_tags))}\n",
        "    ix_to_tag = dict([(value, key) for key, value in tag_to_ix.items()])\n",
        "\n",
        "    word_to_ix[PAD_WORD] = len(word_to_ix)\n",
        "    data = create_data(df)\n",
        "\n",
        "    return word_to_ix, (tag_to_ix, ix_to_tag), data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-97lv_qfdTSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_data(df):\n",
        "    sentences = []\n",
        "\n",
        "    doc_ids = list(set(df.doc))\n",
        "    for doc_id in doc_ids:\n",
        "        sent_ids = list(set(df[df.doc == doc_id].sentence))\n",
        "        df_docs = df[df.doc == doc_id]\n",
        "        for sent_id in sent_ids:\n",
        "            df_sents = df_docs[df_docs.sentence == sent_id]\n",
        "\n",
        "            words = [word for word in df_sents.word]\n",
        "            tags = [tag for tag in df_sents.type]\n",
        "\n",
        "            sentences.append((words, tags))\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4t0xHNMdgqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embed_weights(target_vocab, ft):\n",
        "    '''\n",
        "        For each word in dataset’s vocabulary, we check if it is vocabulary.\n",
        "        If it is, we load its pre-trained word vector.\n",
        "        Otherwise, we initialize a random vector.\n",
        "    '''\n",
        "    matrix_len = len(target_vocab)\n",
        "    weights_matrix = np.zeros((matrix_len, EMBED_DIM))\n",
        "    words_found = 0\n",
        "\n",
        "    for word, ix in target_vocab.items():\n",
        "        try:\n",
        "            weights_matrix[ix] = ft.get_word_vector(word)\n",
        "            words_found += 1\n",
        "        except KeyError:\n",
        "            weights_matrix[ix] = np.random.normal(scale=0.6, size=(EMBED_DIM, ))\n",
        "\n",
        "    return torch.as_tensor(weights_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIN54TXCQ5Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embed_layer(weights_matrix, non_trainable=True):\n",
        "    '''\n",
        "        custom embedding layer\n",
        "    '''\n",
        "    num_embeds, embed_dim = weights_matrix.size()\n",
        "    embed_layer = nn.Embedding(num_embeds, embed_dim)\n",
        "    embed_layer.load_state_dict({'weight': weights_matrix})\n",
        "    if non_trainable:\n",
        "        embed_layer.weight.requires_grad = False\n",
        "\n",
        "    return embed_layer, num_embeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N03eD_bp0ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_indices(sentence, word_to_ix):\n",
        "    '''\n",
        "        retrieves indices of sentence from word vocabulary\n",
        "    '''\n",
        "    word_indices = []\n",
        "    for token in sentence:\n",
        "        word_indices.append(word_to_ix[token])\n",
        "    return word_indices\n",
        "\n",
        "\n",
        "def get_batch(batch, word_to_ix, tag_to_ix):\n",
        "    '''\n",
        "        creates batches of indices for words and tags\n",
        "        finds maximum length of sentence for padding\n",
        "    '''\n",
        "    words_batch, tags_batch = [], []\n",
        "    max_sent_len = 0\n",
        "\n",
        "    sentences_batch, tag_batch = batch\n",
        "    for i in range(len(sentences_batch)):\n",
        "        sentence = sentences_batch[i].split(' ')\n",
        "        tags = tag_batch[i].split()\n",
        "\n",
        "        word_indices = get_indices(sentence, word_to_ix)\n",
        "        tag_indices = [tag_to_ix[tag] for tag in tags]\n",
        "\n",
        "        words_batch.append(word_indices)\n",
        "        tags_batch.append(tag_indices)\n",
        "\n",
        "        max_sent_len = max(max_sent_len, len(sentence))\n",
        "\n",
        "    return (words_batch, tags_batch), max_sent_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d688kMW14d5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_indices(indices, word_to_ix, length):\n",
        "    '''\n",
        "        performs padding of word and tag batches\n",
        "\n",
        "        indices:\n",
        "                indices[0] - word_indices\n",
        "                indices[1] - tag_indices\n",
        "    '''\n",
        "    for i in range(len(indices[0])):\n",
        "        for j in range(len(indices[0][i]), length):\n",
        "            indices[0][i].append(word_to_ix[PAD_WORD])\n",
        "            indices[1][i].append(PAD_TAG)\n",
        "        indices[0][i] = torch.tensor(indices[0][i], dtype=torch.long)\n",
        "        indices[1][i] = torch.tensor(indices[1][i], dtype=torch.long)\n",
        "\n",
        "    return (torch.stack(indices[0]), torch.stack(indices[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2vJEP38Zlp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(df, ft, doc_id=0, continue_training=False):\n",
        "    start = datetime.now()\n",
        "    # reads vocabulary and creates corpus\n",
        "    # :tag_dics: - (tag_to_ix, ix_to_tag)\n",
        "    # :data: - [([token, token, ...], [id, id, ...])]\n",
        "    word_to_ix, tag_dicts, data = create_vocabulary(df)    \n",
        "\n",
        "    # initialize wight embeddings from fasttext\n",
        "    print('create weight matrix')\n",
        "    weights_matrix = create_embed_weights(word_to_ix, ft)\n",
        "    # initialize model\n",
        "    # layers - Embeddings -> Bi-gru -> Dense (softmax)\n",
        "    model = Event_tagger(word_to_ix, tag_dicts[0], tag_dicts[1],\n",
        "                         weights_matrix, BATCH_SIZE)\n",
        "\n",
        "    # loss function and optimizer\n",
        "    loss_function = nn.CrossEntropyLoss(ignore_index=PAD_TAG)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    if continue_training:\n",
        "        model_path = '/content/drive/My Drive/' + str(doc_id - 1) + '_model-2.pt'\n",
        "        model, optimizer, start_epoch = load_model(model_path, model, optimizer)\n",
        "\n",
        "    \n",
        "    print('start...')\n",
        "    # helping function to crate DataLoader for creating batches\n",
        "    corpus = Corpus(word_to_ix, tag_dicts[0], data)\n",
        "    # create DataLoader object and train model\n",
        "    dataloader = DataLoader(dataset=corpus,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            shuffle=False,\n",
        "                            num_workers=8)\n",
        "    for epoch in range(EPOCHS):\n",
        "        for batch_index, batch in enumerate(dataloader):\n",
        "            # Clear gradients and hidden layer\n",
        "            model.zero_grad()\n",
        "\n",
        "            # batch_indices - (words_batch, tags_batch)\n",
        "            batch_indices, length = get_batch(batch, word_to_ix, tag_dicts[0])\n",
        "            batch_indices = pad_indices(batch_indices, word_to_ix, length)\n",
        "\n",
        "            tag_scores_batch = model(batch_indices[0])\n",
        "            target = batch_indices[1].view(-1)\n",
        "            output = tag_scores_batch.view(-1, tag_scores_batch.size()[-1])\n",
        "\n",
        "            loss = loss_function(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # printing part\n",
        "            if batch_index == 0 or (batch_index + 1) % 10 == 0 or batch_index + 1 == math.ceil(\n",
        "                    len(data) / BATCH_SIZE):\n",
        "                predicted_tags = []\n",
        "                y_true = []\n",
        "                for i in range(output.size()[0]):\n",
        "                    if target[i].item() == PAD_TAG:\n",
        "                        continue\n",
        "                    # calculate accuracy every 1000 batches\n",
        "                    # and on the last batch\n",
        "                    _, predicted_ix = torch.max(output[i], 0)\n",
        "                    predicted_tags.append(predicted_ix.item())\n",
        "                    y_true.append(target[i].item())\n",
        "                score = f1_score(y_true, predicted_tags, average='weighted')\n",
        "                print(\n",
        "                    \"Epoch {}/{} | Batch {}/{} | Loss {:.3f} | F1 score {:.3f}\"\n",
        "                    .format(epoch + 1, EPOCHS, batch_index + 1,\n",
        "                            math.ceil(len(data) / BATCH_SIZE),\n",
        "                            loss.data.item(), score))\n",
        "    checkpoint = {\n",
        "        'epoch': doc_id * epoch + 1,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "    save_model(checkpoint, False, str(doc_id), 'best_model')\n",
        "    print('model saved')\n",
        "    doc_id += 1\n",
        "    end = datetime.now()\n",
        "    print('Finished... {}'.format(end - start))\n",
        "\n",
        "    return model, doc_id, word_to_ix, tag_dicts[0], tag_dicts[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Veixlf10-1lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/output.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC4rdN4hTa5j",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Do3PuCCTdB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_train_split(df, ratio=.2):\n",
        "    max_doc = df.iloc[-1].doc\n",
        "    # :ratio: - percentage of data left to testing\n",
        "    split_doc = max_doc * (1 - ratio)\n",
        "\n",
        "    df_train = df[df.doc <= split_doc]\n",
        "    df_test = df[df.doc > split_doc]\n",
        "\n",
        "    return df_train, df_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mZCEb2EXW5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_sentences(df, word_to_ix, tag_to_ix, ix_to_tag, doc_id):\n",
        "    batch_size = 1\n",
        "\n",
        "    print('create weight matrix')\n",
        "    weights_matrix = create_embed_weights(word_to_ix, ft)\n",
        "    model = Event_tagger(word_to_ix, tag_to_ix, ix_to_tag,\n",
        "                         weights_matrix, BATCH_SIZE)\n",
        "\n",
        "    # loss function and optimizer\n",
        "    # loss_function = nn.CrossEntropyLoss(ignore_index=PAD_TAG)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # load model\n",
        "    model_path = 'models/' + str(doc_id - 2) + '_model.pt'\n",
        "    model, optimizer, start_epoch = load_model(model_path, model, optimizer)\n",
        "    model.gru_hidden_embeds = model.init_hidden_embeddings(batch_size)\n",
        "    model.gru.flatten_parameters()\n",
        "\n",
        "    # load test dataset\n",
        "    data = create_data(df)\n",
        "\n",
        "    while doc_id < max(data.keys()):\n",
        "        word_indices = sentences_to_indices(data[doc_id], word_to_ix)\n",
        "\n",
        "        # run test dataset through model\n",
        "        predicted_tags = []\n",
        "        y_true = []\n",
        "        for i in range(len(word_indices)):\n",
        "            tag_scores = model(torch.stack([word_indices[i]]))\n",
        "            out_probs = torch.squeeze(tag_scores)\n",
        "            predicted_tags = []\n",
        "            for pset in out_probs:\n",
        "                _, predicted_ix = torch.max(pset, 0)\n",
        "                predicted_tags.append(predicted_ix.item())\n",
        "            target = [tag_to_ix[tag] for tag in data[doc_id][i][1]]\n",
        "            y_true.extend(target)\n",
        "        score = f1_score(y_true, predicted_tags)\n",
        "        print(\"Testing... Doc id {}/{} | F1 score {:.3f}\".format(doc_id,\n",
        "                                                                 max(data.keys()),\n",
        "                                                                 score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyq6zQ-mcJiC",
        "colab_type": "text"
      },
      "source": [
        "# Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzJRELr4cIxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentences_to_indices(sentences, word_to_ix):\n",
        "    '''\n",
        "        sentences - [[token, ...], [token, ...], ...]\n",
        "        converts sentences to word indices\n",
        "    '''\n",
        "    all_word_indices = []\n",
        "    for sent in sentences:\n",
        "        word_indices = get_indices_train(sent[0], word_to_ix)\n",
        "        all_word_indices.append(word_indices)\n",
        "    return all_word_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LMoj6IIcbQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_indices_train(sentence, word_to_ix):\n",
        "    '''\n",
        "        retrieves indices of sentence from helpping dictionaries,\n",
        "        maximum length of word in sentence\n",
        "    '''\n",
        "    word_indices = []\n",
        "    for token in sentence:\n",
        "        # read word index from dictionary\n",
        "        # if word is not in the dictionary treat as unknown word\n",
        "        tok = token if token in word_to_ix else PAD_WORD\n",
        "        word_indices.append(word_to_ix[tok])\n",
        "\n",
        "    return torch.tensor(word_indices, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR3jWvUTc2Ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_tags_to_tokens(tokens, tags):\n",
        "    '''\n",
        "        tokens - [token, token, ...]\n",
        "        tags   - [tag, tag, ...]\n",
        "\n",
        "        concatenate tokens and tags into token/tag format as was in\n",
        "        original corpus\n",
        "    '''\n",
        "    tokens = [tokens[0][i] + '/' + tags[i] + '/' + tokens[1][i] for i in range(len(tokens[0]))]\n",
        "    return ' '.join(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EuhW-T4dXJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_answer(out_file, output):\n",
        "    '''\n",
        "        output - [string, string, ...]\n",
        "        writes output to the file\n",
        "    '''\n",
        "    with open(out_file, 'w') as f:\n",
        "        for item in output:\n",
        "            f.write(\"%s\\n\" % item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq_Fh5Q1daZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tag_sentence(df, word_to_ix, tag_to_ix, ix_to_tag, doc_id):\n",
        "    batch_size = 1\n",
        "\n",
        "    print('create weight matrix')\n",
        "    weights_matrix = create_embed_weights(word_to_ix, ft)\n",
        "    model = Event_tagger(word_to_ix, tag_to_ix, ix_to_tag,\n",
        "                         weights_matrix, BATCH_SIZE)\n",
        "\n",
        "    # loss function and optimizer\n",
        "    # loss_function = nn.CrossEntropyLoss(ignore_index=PAD_TAG)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # load model\n",
        "    model_path = str(doc_id) + '/model.pt'\n",
        "    model, optimizer, start_epoch = load_model(model_path, model, optimizer)\n",
        "    model.gru_hidden_embeds = model.init_hidden_embeddings(batch_size)\n",
        "    model.gru.flatten_parameters()\n",
        "\n",
        "    # load test dataset\n",
        "    data = create_data(df)\n",
        "\n",
        "    word_indices = sentences_to_indices(data[doc_id], word_to_ix)\n",
        "\n",
        "    # run test dataset through model\n",
        "    output = []\n",
        "    for i in range(len(word_indices)):\n",
        "        tag_scores = model(torch.stack([word_indices[i]]))\n",
        "        out_probs = torch.squeeze(tag_scores)\n",
        "        predicted_tags = []\n",
        "        for pset in out_probs:\n",
        "            _, predicted_ix = torch.max(pset, 0)\n",
        "            predicted_tags.append(ix_to_tag[predicted_ix.item()])\n",
        "        output.append(add_tags_to_tokens(data[doc_id][i], predicted_tags))\n",
        "\n",
        "    # Save answer\n",
        "    out_file = str(doc_id) + '.txt'\n",
        "    save_answer(out_file, output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MStrNqZpTSfV",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1v0opq-ZgEw",
        "colab_type": "code",
        "outputId": "b33e08d8-78a9-488b-a0a6-81bbf078b990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df_train, df_test = test_train_split(df)\n",
        "\n",
        "model, doc_id, word_to_ix, tag_to_ix, ix_to_tag = train_model(df_train, ft)\n",
        "# test_sentences(df_test, word_to_ix, tag_to_ix, ix_to_tag, doc_id + 1)\n",
        "\n",
        "# print(doc_id)\n",
        "# tag_sentence(df, word_to_ix, tag_to_ix, ix_to_tag, doc_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "create weight matrix\n",
            "start...\n",
            "Epoch 1/50 | Batch 1/32 | Loss 3.976 | F1 score 0.007\n",
            "Epoch 1/50 | Batch 10/32 | Loss 1.468 | F1 score 0.541\n",
            "Epoch 1/50 | Batch 20/32 | Loss 1.223 | F1 score 0.611\n",
            "Epoch 1/50 | Batch 30/32 | Loss 1.439 | F1 score 0.525\n",
            "Epoch 1/50 | Batch 32/32 | Loss 1.129 | F1 score 0.643\n",
            "Epoch 2/50 | Batch 1/32 | Loss 1.386 | F1 score 0.536\n",
            "Epoch 2/50 | Batch 10/32 | Loss 1.134 | F1 score 0.603\n",
            "Epoch 2/50 | Batch 20/32 | Loss 0.927 | F1 score 0.673\n",
            "Epoch 2/50 | Batch 30/32 | Loss 1.174 | F1 score 0.607\n",
            "Epoch 2/50 | Batch 32/32 | Loss 0.888 | F1 score 0.714\n",
            "Epoch 3/50 | Batch 1/32 | Loss 1.131 | F1 score 0.624\n",
            "Epoch 3/50 | Batch 10/32 | Loss 0.883 | F1 score 0.677\n",
            "Epoch 3/50 | Batch 20/32 | Loss 0.731 | F1 score 0.754\n",
            "Epoch 3/50 | Batch 30/32 | Loss 0.987 | F1 score 0.688\n",
            "Epoch 3/50 | Batch 32/32 | Loss 0.748 | F1 score 0.770\n",
            "Epoch 4/50 | Batch 1/32 | Loss 0.928 | F1 score 0.695\n",
            "Epoch 4/50 | Batch 10/32 | Loss 0.730 | F1 score 0.743\n",
            "Epoch 4/50 | Batch 20/32 | Loss 0.627 | F1 score 0.783\n",
            "Epoch 4/50 | Batch 30/32 | Loss 0.880 | F1 score 0.725\n",
            "Epoch 4/50 | Batch 32/32 | Loss 0.666 | F1 score 0.797\n",
            "Epoch 5/50 | Batch 1/32 | Loss 0.803 | F1 score 0.735\n",
            "Epoch 5/50 | Batch 10/32 | Loss 0.636 | F1 score 0.781\n",
            "Epoch 5/50 | Batch 20/32 | Loss 0.562 | F1 score 0.812\n",
            "Epoch 5/50 | Batch 30/32 | Loss 0.812 | F1 score 0.748\n",
            "Epoch 5/50 | Batch 32/32 | Loss 0.597 | F1 score 0.817\n",
            "Epoch 6/50 | Batch 1/32 | Loss 0.728 | F1 score 0.756\n",
            "Epoch 6/50 | Batch 10/32 | Loss 0.583 | F1 score 0.807\n",
            "Epoch 6/50 | Batch 20/32 | Loss 0.515 | F1 score 0.832\n",
            "Epoch 6/50 | Batch 30/32 | Loss 0.757 | F1 score 0.767\n",
            "Epoch 6/50 | Batch 32/32 | Loss 0.547 | F1 score 0.834\n",
            "Epoch 7/50 | Batch 1/32 | Loss 0.671 | F1 score 0.771\n",
            "Epoch 7/50 | Batch 10/32 | Loss 0.539 | F1 score 0.824\n",
            "Epoch 7/50 | Batch 20/32 | Loss 0.478 | F1 score 0.843\n",
            "Epoch 7/50 | Batch 30/32 | Loss 0.727 | F1 score 0.774\n",
            "Epoch 7/50 | Batch 32/32 | Loss 0.510 | F1 score 0.846\n",
            "Epoch 8/50 | Batch 1/32 | Loss 0.627 | F1 score 0.784\n",
            "Epoch 8/50 | Batch 10/32 | Loss 0.503 | F1 score 0.833\n",
            "Epoch 8/50 | Batch 20/32 | Loss 0.456 | F1 score 0.854\n",
            "Epoch 8/50 | Batch 30/32 | Loss 0.688 | F1 score 0.786\n",
            "Epoch 8/50 | Batch 32/32 | Loss 0.476 | F1 score 0.860\n",
            "Epoch 9/50 | Batch 1/32 | Loss 0.585 | F1 score 0.798\n",
            "Epoch 9/50 | Batch 10/32 | Loss 0.479 | F1 score 0.841\n",
            "Epoch 9/50 | Batch 20/32 | Loss 0.432 | F1 score 0.861\n",
            "Epoch 9/50 | Batch 30/32 | Loss 0.648 | F1 score 0.800\n",
            "Epoch 9/50 | Batch 32/32 | Loss 0.454 | F1 score 0.864\n",
            "Epoch 10/50 | Batch 1/32 | Loss 0.546 | F1 score 0.814\n",
            "Epoch 10/50 | Batch 10/32 | Loss 0.456 | F1 score 0.849\n",
            "Epoch 10/50 | Batch 20/32 | Loss 0.413 | F1 score 0.867\n",
            "Epoch 10/50 | Batch 30/32 | Loss 0.617 | F1 score 0.804\n",
            "Epoch 10/50 | Batch 32/32 | Loss 0.429 | F1 score 0.872\n",
            "Epoch 11/50 | Batch 1/32 | Loss 0.520 | F1 score 0.821\n",
            "Epoch 11/50 | Batch 10/32 | Loss 0.434 | F1 score 0.855\n",
            "Epoch 11/50 | Batch 20/32 | Loss 0.398 | F1 score 0.871\n",
            "Epoch 11/50 | Batch 30/32 | Loss 0.589 | F1 score 0.813\n",
            "Epoch 11/50 | Batch 32/32 | Loss 0.406 | F1 score 0.876\n",
            "Epoch 12/50 | Batch 1/32 | Loss 0.501 | F1 score 0.829\n",
            "Epoch 12/50 | Batch 10/32 | Loss 0.417 | F1 score 0.860\n",
            "Epoch 12/50 | Batch 20/32 | Loss 0.385 | F1 score 0.877\n",
            "Epoch 12/50 | Batch 30/32 | Loss 0.565 | F1 score 0.816\n",
            "Epoch 12/50 | Batch 32/32 | Loss 0.399 | F1 score 0.876\n",
            "Epoch 13/50 | Batch 1/32 | Loss 0.488 | F1 score 0.833\n",
            "Epoch 13/50 | Batch 10/32 | Loss 0.401 | F1 score 0.865\n",
            "Epoch 13/50 | Batch 20/32 | Loss 0.372 | F1 score 0.881\n",
            "Epoch 13/50 | Batch 30/32 | Loss 0.547 | F1 score 0.823\n",
            "Epoch 13/50 | Batch 32/32 | Loss 0.378 | F1 score 0.883\n",
            "Epoch 14/50 | Batch 1/32 | Loss 0.470 | F1 score 0.844\n",
            "Epoch 14/50 | Batch 10/32 | Loss 0.384 | F1 score 0.875\n",
            "Epoch 14/50 | Batch 20/32 | Loss 0.358 | F1 score 0.886\n",
            "Epoch 14/50 | Batch 30/32 | Loss 0.530 | F1 score 0.826\n",
            "Epoch 14/50 | Batch 32/32 | Loss 0.366 | F1 score 0.884\n",
            "Epoch 15/50 | Batch 1/32 | Loss 0.453 | F1 score 0.846\n",
            "Epoch 15/50 | Batch 10/32 | Loss 0.372 | F1 score 0.878\n",
            "Epoch 15/50 | Batch 20/32 | Loss 0.343 | F1 score 0.889\n",
            "Epoch 15/50 | Batch 30/32 | Loss 0.512 | F1 score 0.831\n",
            "Epoch 15/50 | Batch 32/32 | Loss 0.356 | F1 score 0.884\n",
            "Epoch 16/50 | Batch 1/32 | Loss 0.444 | F1 score 0.850\n",
            "Epoch 16/50 | Batch 10/32 | Loss 0.359 | F1 score 0.883\n",
            "Epoch 16/50 | Batch 20/32 | Loss 0.334 | F1 score 0.894\n",
            "Epoch 16/50 | Batch 30/32 | Loss 0.495 | F1 score 0.836\n",
            "Epoch 16/50 | Batch 32/32 | Loss 0.346 | F1 score 0.888\n",
            "Epoch 17/50 | Batch 1/32 | Loss 0.432 | F1 score 0.856\n",
            "Epoch 17/50 | Batch 10/32 | Loss 0.347 | F1 score 0.887\n",
            "Epoch 17/50 | Batch 20/32 | Loss 0.327 | F1 score 0.892\n",
            "Epoch 17/50 | Batch 30/32 | Loss 0.484 | F1 score 0.836\n",
            "Epoch 17/50 | Batch 32/32 | Loss 0.336 | F1 score 0.890\n",
            "Epoch 18/50 | Batch 1/32 | Loss 0.418 | F1 score 0.859\n",
            "Epoch 18/50 | Batch 10/32 | Loss 0.339 | F1 score 0.890\n",
            "Epoch 18/50 | Batch 20/32 | Loss 0.319 | F1 score 0.897\n",
            "Epoch 18/50 | Batch 30/32 | Loss 0.472 | F1 score 0.844\n",
            "Epoch 18/50 | Batch 32/32 | Loss 0.325 | F1 score 0.894\n",
            "Epoch 19/50 | Batch 1/32 | Loss 0.409 | F1 score 0.861\n",
            "Epoch 19/50 | Batch 10/32 | Loss 0.333 | F1 score 0.894\n",
            "Epoch 19/50 | Batch 20/32 | Loss 0.311 | F1 score 0.899\n",
            "Epoch 19/50 | Batch 30/32 | Loss 0.464 | F1 score 0.848\n",
            "Epoch 19/50 | Batch 32/32 | Loss 0.309 | F1 score 0.899\n",
            "Epoch 20/50 | Batch 1/32 | Loss 0.396 | F1 score 0.865\n",
            "Epoch 20/50 | Batch 10/32 | Loss 0.325 | F1 score 0.896\n",
            "Epoch 20/50 | Batch 20/32 | Loss 0.302 | F1 score 0.901\n",
            "Epoch 20/50 | Batch 30/32 | Loss 0.448 | F1 score 0.850\n",
            "Epoch 20/50 | Batch 32/32 | Loss 0.309 | F1 score 0.896\n",
            "Epoch 21/50 | Batch 1/32 | Loss 0.386 | F1 score 0.867\n",
            "Epoch 21/50 | Batch 10/32 | Loss 0.310 | F1 score 0.901\n",
            "Epoch 21/50 | Batch 20/32 | Loss 0.295 | F1 score 0.903\n",
            "Epoch 21/50 | Batch 30/32 | Loss 0.437 | F1 score 0.855\n",
            "Epoch 21/50 | Batch 32/32 | Loss 0.301 | F1 score 0.898\n",
            "Epoch 22/50 | Batch 1/32 | Loss 0.377 | F1 score 0.872\n",
            "Epoch 22/50 | Batch 10/32 | Loss 0.305 | F1 score 0.902\n",
            "Epoch 22/50 | Batch 20/32 | Loss 0.289 | F1 score 0.906\n",
            "Epoch 22/50 | Batch 30/32 | Loss 0.426 | F1 score 0.858\n",
            "Epoch 22/50 | Batch 32/32 | Loss 0.290 | F1 score 0.902\n",
            "Epoch 23/50 | Batch 1/32 | Loss 0.366 | F1 score 0.872\n",
            "Epoch 23/50 | Batch 10/32 | Loss 0.298 | F1 score 0.901\n",
            "Epoch 23/50 | Batch 20/32 | Loss 0.282 | F1 score 0.907\n",
            "Epoch 23/50 | Batch 30/32 | Loss 0.418 | F1 score 0.859\n",
            "Epoch 23/50 | Batch 32/32 | Loss 0.278 | F1 score 0.903\n",
            "Epoch 24/50 | Batch 1/32 | Loss 0.357 | F1 score 0.880\n",
            "Epoch 24/50 | Batch 10/32 | Loss 0.292 | F1 score 0.905\n",
            "Epoch 24/50 | Batch 20/32 | Loss 0.273 | F1 score 0.909\n",
            "Epoch 24/50 | Batch 30/32 | Loss 0.405 | F1 score 0.864\n",
            "Epoch 24/50 | Batch 32/32 | Loss 0.278 | F1 score 0.903\n",
            "Epoch 25/50 | Batch 1/32 | Loss 0.348 | F1 score 0.881\n",
            "Epoch 25/50 | Batch 10/32 | Loss 0.285 | F1 score 0.906\n",
            "Epoch 25/50 | Batch 20/32 | Loss 0.266 | F1 score 0.911\n",
            "Epoch 25/50 | Batch 30/32 | Loss 0.392 | F1 score 0.869\n",
            "Epoch 25/50 | Batch 32/32 | Loss 0.270 | F1 score 0.910\n",
            "Epoch 26/50 | Batch 1/32 | Loss 0.343 | F1 score 0.883\n",
            "Epoch 26/50 | Batch 10/32 | Loss 0.284 | F1 score 0.905\n",
            "Epoch 26/50 | Batch 20/32 | Loss 0.258 | F1 score 0.912\n",
            "Epoch 26/50 | Batch 30/32 | Loss 0.386 | F1 score 0.868\n",
            "Epoch 26/50 | Batch 32/32 | Loss 0.260 | F1 score 0.910\n",
            "Epoch 27/50 | Batch 1/32 | Loss 0.334 | F1 score 0.888\n",
            "Epoch 27/50 | Batch 10/32 | Loss 0.277 | F1 score 0.906\n",
            "Epoch 27/50 | Batch 20/32 | Loss 0.259 | F1 score 0.912\n",
            "Epoch 27/50 | Batch 30/32 | Loss 0.378 | F1 score 0.870\n",
            "Epoch 27/50 | Batch 32/32 | Loss 0.258 | F1 score 0.913\n",
            "Epoch 28/50 | Batch 1/32 | Loss 0.332 | F1 score 0.889\n",
            "Epoch 28/50 | Batch 10/32 | Loss 0.272 | F1 score 0.912\n",
            "Epoch 28/50 | Batch 20/32 | Loss 0.250 | F1 score 0.919\n",
            "Epoch 28/50 | Batch 30/32 | Loss 0.366 | F1 score 0.880\n",
            "Epoch 28/50 | Batch 32/32 | Loss 0.244 | F1 score 0.914\n",
            "Epoch 29/50 | Batch 1/32 | Loss 0.317 | F1 score 0.889\n",
            "Epoch 29/50 | Batch 10/32 | Loss 0.269 | F1 score 0.913\n",
            "Epoch 29/50 | Batch 20/32 | Loss 0.244 | F1 score 0.919\n",
            "Epoch 29/50 | Batch 30/32 | Loss 0.362 | F1 score 0.879\n",
            "Epoch 29/50 | Batch 32/32 | Loss 0.242 | F1 score 0.914\n",
            "Epoch 30/50 | Batch 1/32 | Loss 0.309 | F1 score 0.895\n",
            "Epoch 30/50 | Batch 10/32 | Loss 0.263 | F1 score 0.912\n",
            "Epoch 30/50 | Batch 20/32 | Loss 0.238 | F1 score 0.923\n",
            "Epoch 30/50 | Batch 30/32 | Loss 0.357 | F1 score 0.883\n",
            "Epoch 30/50 | Batch 32/32 | Loss 0.236 | F1 score 0.918\n",
            "Epoch 31/50 | Batch 1/32 | Loss 0.300 | F1 score 0.896\n",
            "Epoch 31/50 | Batch 10/32 | Loss 0.252 | F1 score 0.916\n",
            "Epoch 31/50 | Batch 20/32 | Loss 0.241 | F1 score 0.922\n",
            "Epoch 31/50 | Batch 30/32 | Loss 0.344 | F1 score 0.886\n",
            "Epoch 31/50 | Batch 32/32 | Loss 0.236 | F1 score 0.920\n",
            "Epoch 32/50 | Batch 1/32 | Loss 0.296 | F1 score 0.899\n",
            "Epoch 32/50 | Batch 10/32 | Loss 0.241 | F1 score 0.920\n",
            "Epoch 32/50 | Batch 20/32 | Loss 0.233 | F1 score 0.926\n",
            "Epoch 32/50 | Batch 30/32 | Loss 0.340 | F1 score 0.888\n",
            "Epoch 32/50 | Batch 32/32 | Loss 0.225 | F1 score 0.924\n",
            "Epoch 33/50 | Batch 1/32 | Loss 0.293 | F1 score 0.899\n",
            "Epoch 33/50 | Batch 10/32 | Loss 0.235 | F1 score 0.925\n",
            "Epoch 33/50 | Batch 20/32 | Loss 0.223 | F1 score 0.927\n",
            "Epoch 33/50 | Batch 30/32 | Loss 0.322 | F1 score 0.893\n",
            "Epoch 33/50 | Batch 32/32 | Loss 0.225 | F1 score 0.922\n",
            "Epoch 34/50 | Batch 1/32 | Loss 0.289 | F1 score 0.904\n",
            "Epoch 34/50 | Batch 10/32 | Loss 0.230 | F1 score 0.925\n",
            "Epoch 34/50 | Batch 20/32 | Loss 0.219 | F1 score 0.928\n",
            "Epoch 34/50 | Batch 30/32 | Loss 0.315 | F1 score 0.896\n",
            "Epoch 34/50 | Batch 32/32 | Loss 0.211 | F1 score 0.928\n",
            "Epoch 35/50 | Batch 1/32 | Loss 0.275 | F1 score 0.905\n",
            "Epoch 35/50 | Batch 10/32 | Loss 0.223 | F1 score 0.923\n",
            "Epoch 35/50 | Batch 20/32 | Loss 0.214 | F1 score 0.929\n",
            "Epoch 35/50 | Batch 30/32 | Loss 0.309 | F1 score 0.896\n",
            "Epoch 35/50 | Batch 32/32 | Loss 0.217 | F1 score 0.927\n",
            "Epoch 36/50 | Batch 1/32 | Loss 0.274 | F1 score 0.907\n",
            "Epoch 36/50 | Batch 10/32 | Loss 0.221 | F1 score 0.926\n",
            "Epoch 36/50 | Batch 20/32 | Loss 0.206 | F1 score 0.933\n",
            "Epoch 36/50 | Batch 30/32 | Loss 0.306 | F1 score 0.898\n",
            "Epoch 36/50 | Batch 32/32 | Loss 0.209 | F1 score 0.929\n",
            "Epoch 37/50 | Batch 1/32 | Loss 0.274 | F1 score 0.902\n",
            "Epoch 37/50 | Batch 10/32 | Loss 0.223 | F1 score 0.927\n",
            "Epoch 37/50 | Batch 20/32 | Loss 0.210 | F1 score 0.930\n",
            "Epoch 37/50 | Batch 30/32 | Loss 0.300 | F1 score 0.896\n",
            "Epoch 37/50 | Batch 32/32 | Loss 0.204 | F1 score 0.926\n",
            "Epoch 38/50 | Batch 1/32 | Loss 0.267 | F1 score 0.908\n",
            "Epoch 38/50 | Batch 10/32 | Loss 0.212 | F1 score 0.929\n",
            "Epoch 38/50 | Batch 20/32 | Loss 0.191 | F1 score 0.939\n",
            "Epoch 38/50 | Batch 30/32 | Loss 0.316 | F1 score 0.890\n",
            "Epoch 38/50 | Batch 32/32 | Loss 0.199 | F1 score 0.930\n",
            "Epoch 39/50 | Batch 1/32 | Loss 0.261 | F1 score 0.910\n",
            "Epoch 39/50 | Batch 10/32 | Loss 0.215 | F1 score 0.930\n",
            "Epoch 39/50 | Batch 20/32 | Loss 0.187 | F1 score 0.939\n",
            "Epoch 39/50 | Batch 30/32 | Loss 0.288 | F1 score 0.902\n",
            "Epoch 39/50 | Batch 32/32 | Loss 0.205 | F1 score 0.930\n",
            "Epoch 40/50 | Batch 1/32 | Loss 0.256 | F1 score 0.915\n",
            "Epoch 40/50 | Batch 10/32 | Loss 0.220 | F1 score 0.927\n",
            "Epoch 40/50 | Batch 20/32 | Loss 0.194 | F1 score 0.934\n",
            "Epoch 40/50 | Batch 30/32 | Loss 0.281 | F1 score 0.901\n",
            "Epoch 40/50 | Batch 32/32 | Loss 0.180 | F1 score 0.934\n",
            "Epoch 41/50 | Batch 1/32 | Loss 0.242 | F1 score 0.916\n",
            "Epoch 41/50 | Batch 10/32 | Loss 0.203 | F1 score 0.932\n",
            "Epoch 41/50 | Batch 20/32 | Loss 0.181 | F1 score 0.942\n",
            "Epoch 41/50 | Batch 30/32 | Loss 0.286 | F1 score 0.906\n",
            "Epoch 41/50 | Batch 32/32 | Loss 0.184 | F1 score 0.933\n",
            "Epoch 42/50 | Batch 1/32 | Loss 0.239 | F1 score 0.918\n",
            "Epoch 42/50 | Batch 10/32 | Loss 0.193 | F1 score 0.934\n",
            "Epoch 42/50 | Batch 20/32 | Loss 0.172 | F1 score 0.943\n",
            "Epoch 42/50 | Batch 30/32 | Loss 0.267 | F1 score 0.908\n",
            "Epoch 42/50 | Batch 32/32 | Loss 0.181 | F1 score 0.935\n",
            "Epoch 43/50 | Batch 1/32 | Loss 0.232 | F1 score 0.920\n",
            "Epoch 43/50 | Batch 10/32 | Loss 0.187 | F1 score 0.939\n",
            "Epoch 43/50 | Batch 20/32 | Loss 0.163 | F1 score 0.946\n",
            "Epoch 43/50 | Batch 30/32 | Loss 0.249 | F1 score 0.916\n",
            "Epoch 43/50 | Batch 32/32 | Loss 0.177 | F1 score 0.939\n",
            "Epoch 44/50 | Batch 1/32 | Loss 0.233 | F1 score 0.921\n",
            "Epoch 44/50 | Batch 10/32 | Loss 0.182 | F1 score 0.940\n",
            "Epoch 44/50 | Batch 20/32 | Loss 0.156 | F1 score 0.948\n",
            "Epoch 44/50 | Batch 30/32 | Loss 0.246 | F1 score 0.913\n",
            "Epoch 44/50 | Batch 32/32 | Loss 0.167 | F1 score 0.943\n",
            "Epoch 45/50 | Batch 1/32 | Loss 0.220 | F1 score 0.925\n",
            "Epoch 45/50 | Batch 10/32 | Loss 0.173 | F1 score 0.944\n",
            "Epoch 45/50 | Batch 20/32 | Loss 0.147 | F1 score 0.951\n",
            "Epoch 45/50 | Batch 30/32 | Loss 0.239 | F1 score 0.920\n",
            "Epoch 45/50 | Batch 32/32 | Loss 0.162 | F1 score 0.944\n",
            "Epoch 46/50 | Batch 1/32 | Loss 0.210 | F1 score 0.928\n",
            "Epoch 46/50 | Batch 10/32 | Loss 0.165 | F1 score 0.946\n",
            "Epoch 46/50 | Batch 20/32 | Loss 0.143 | F1 score 0.952\n",
            "Epoch 46/50 | Batch 30/32 | Loss 0.221 | F1 score 0.924\n",
            "Epoch 46/50 | Batch 32/32 | Loss 0.151 | F1 score 0.948\n",
            "Epoch 47/50 | Batch 1/32 | Loss 0.198 | F1 score 0.933\n",
            "Epoch 47/50 | Batch 10/32 | Loss 0.157 | F1 score 0.947\n",
            "Epoch 47/50 | Batch 20/32 | Loss 0.140 | F1 score 0.952\n",
            "Epoch 47/50 | Batch 30/32 | Loss 0.216 | F1 score 0.926\n",
            "Epoch 47/50 | Batch 32/32 | Loss 0.149 | F1 score 0.950\n",
            "Epoch 48/50 | Batch 1/32 | Loss 0.196 | F1 score 0.935\n",
            "Epoch 48/50 | Batch 10/32 | Loss 0.146 | F1 score 0.954\n",
            "Epoch 48/50 | Batch 20/32 | Loss 0.135 | F1 score 0.953\n",
            "Epoch 48/50 | Batch 30/32 | Loss 0.211 | F1 score 0.925\n",
            "Epoch 48/50 | Batch 32/32 | Loss 0.144 | F1 score 0.950\n",
            "Epoch 49/50 | Batch 1/32 | Loss 0.187 | F1 score 0.939\n",
            "Epoch 49/50 | Batch 10/32 | Loss 0.145 | F1 score 0.951\n",
            "Epoch 49/50 | Batch 20/32 | Loss 0.130 | F1 score 0.955\n",
            "Epoch 49/50 | Batch 30/32 | Loss 0.206 | F1 score 0.927\n",
            "Epoch 49/50 | Batch 32/32 | Loss 0.139 | F1 score 0.953\n",
            "Epoch 50/50 | Batch 1/32 | Loss 0.186 | F1 score 0.938\n",
            "Epoch 50/50 | Batch 10/32 | Loss 0.139 | F1 score 0.954\n",
            "Epoch 50/50 | Batch 20/32 | Loss 0.123 | F1 score 0.959\n",
            "Epoch 50/50 | Batch 30/32 | Loss 0.199 | F1 score 0.929\n",
            "Epoch 50/50 | Batch 32/32 | Loss 0.133 | F1 score 0.954\n",
            "model saved\n",
            "Finished... 3:56:37.627520\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt056FNxxggS",
        "colab_type": "code",
        "outputId": "0f5d73f7-fea2-43ce-a234-4ecfe72a68b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model.ix_to_tag = ix_to_tag\n",
        "torch.save(model, 'biGRU-dropout-model.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Event_tagger. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7H0UPLTpupY",
        "colab_type": "code",
        "outputId": "a827c59f-d658-4fd6-c89b-a4104268a680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc_id"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10bbo-7apQCE",
        "colab_type": "code",
        "outputId": "be816ade-da6d-46b2-84ce-aa0f836f50e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df_test.doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "303705     889\n",
              "303706     889\n",
              "303707     889\n",
              "303708     889\n",
              "303709     889\n",
              "          ... \n",
              "370740    1110\n",
              "370741    1110\n",
              "370742    1110\n",
              "370743    1110\n",
              "370744    1110\n",
              "Name: doc, Length: 67040, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZyxHtubuV-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, shutil\n",
        "folder = 'models'\n",
        "for filename in os.listdir(folder):\n",
        "    file_path = os.path.join(folder, filename)\n",
        "    if filename != '541_model.pt':\n",
        "        try:\n",
        "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                os.unlink(file_path)\n",
        "            elif os.path.isdir(file_path):\n",
        "                shutil.rmtree(file_path)\n",
        "        except Exception as e:\n",
        "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p8bvYcnXeGz",
        "colab_type": "text"
      },
      "source": [
        "# Some stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbYfbZjE68C5",
        "colab_type": "code",
        "outputId": "602de50d-e1a9-47c4-80a2-854144a3b12c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "'''\n",
        "    creating word embeddings for each word in a sentence\n",
        "    \n",
        "    word_embeddings - dictionary {doc_id: [sentence_num - [word_num, ...], ...]}\n",
        "'''\n",
        "\n",
        "word_embeddings = {}\n",
        "for doc in index:\n",
        "    for sentence in index[doc]:\n",
        "        \n",
        "        if doc not in word_embeddings:\n",
        "            word_embeddings[doc] = []\n",
        "            \n",
        "        word_embeddings[doc].append([ft.get_word_vector(word) for word in sentence])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-5108b5157e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
          ]
        }
      ]
    }
  ]
}